{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk import pos_tag\n",
    "\n",
    "vowel_list = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW']\n",
    "consonant_list = ['P', 'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', 'R', 'S',\n",
    "                  'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "\n",
    "syllable_list = vowel_list + consonant_list\n",
    "\n",
    "\n",
    "# vowel_char_combinations = ['A', 'AA', 'AE', 'AEA', 'AEE', 'AEO', 'AEU', 'AI', 'AIA', 'AIE', 'AII', 'AIO', \n",
    "#                            'AIU', 'AO', 'AOA', 'AOI', 'AOU', 'AOUE', 'AU', 'AUA', 'AUE', 'AUI', 'E', 'EA', \n",
    "#                            'EAU', 'EAUI', 'EE', 'EEA', 'EEI', 'EEU', 'EI', 'EIA', 'EIE', 'EO', 'EOA', 'EOI', \n",
    "#                            'EOU', 'EU', 'EUA', 'EUE', 'EUI', 'I', 'IA', 'IAA', 'IAE', 'IAI', 'IAO', 'IAU', \n",
    "#                            'IE', 'IEA', 'IEI', 'IEU', 'II', 'IIO', 'IO', 'IOA', 'IOI', 'IOU', 'IU', 'O', \n",
    "#                            'OA', 'OAI', 'OE', 'OEA', 'OEI', 'OEU', 'OI', 'OIA', 'OIE', 'OO', 'OOE', 'OOI',\n",
    "#                            'OU', 'OUA', 'OUE', 'OUEI', 'OUI', 'U', 'UA', 'UAA', 'UAI', 'UAU', 'UE', 'UEA',\n",
    "#                            'UEE', 'UEI', 'UEOU', 'UEU', 'UEUI', 'UI', 'UIA', 'UIE', 'UO', 'UOI', 'UOIA', \n",
    "#                            'UOU', 'UU']\n",
    "\n",
    "# consonant_char_list = ['B', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V'\n",
    "#                        'W', 'X', 'Y', 'Z']\n",
    "\n",
    "def is_vowel(v):\n",
    "    return '0' in v or '1' in v or '2' in v\n",
    "\n",
    "def has_v_as_ith_vowel_syllable(p, v, i):\n",
    "    vowels_in_p = [j[:-1] for j in p.split(' ') if is_vowel(j)]\n",
    "    if i > len(vowels_in_p):\n",
    "        return False\n",
    "    return v == vowels_in_p[i-1]\n",
    "\n",
    "def has_c_as_ith_vowel_char_combination(w, c, i):\n",
    "    vowels = {'A', 'E', 'I', 'O', 'U'}\n",
    "    res = []\n",
    "    j = 0\n",
    "    while j < len(w):\n",
    "        l = []\n",
    "        if w[j] in vowels:\n",
    "            while w[j] in vowels:\n",
    "                l.append(w[j])\n",
    "                j += 1\n",
    "                if j >= len(w) - 1:\n",
    "                    break\n",
    "            res.append(''.join(l))\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    if len(res) < i:\n",
    "        return False\n",
    "    return res[i-1] == c\n",
    "\n",
    "def n_syllable_before_ith_vowel_syllable(p, c, i, n):\n",
    "    l = p.split(' ')\n",
    "    count = 0\n",
    "    for j in range(len(l)):\n",
    "        if is_vowel(l[j]):\n",
    "            count += 1\n",
    "            if count == i:\n",
    "                if j <= n-1:\n",
    "                    return False\n",
    "                else:\n",
    "                    ans = l[j-n]\n",
    "                    if is_vowel(ans):\n",
    "                        return ans[:-1] == c\n",
    "                    else:\n",
    "                        return ans == c\n",
    "    return False\n",
    "\n",
    "def n_syllable_after_ith_vowel_syllable(p, c, i, n):\n",
    "    l = p.split(' ')\n",
    "    count = 0\n",
    "    for j in range(len(l)):\n",
    "        if is_vowel(l[j]):\n",
    "            count += 1\n",
    "            if count == i:\n",
    "                if j >= len(l) - n:\n",
    "                    return False\n",
    "                else:\n",
    "                    ans = l[j+n]\n",
    "                    if is_vowel(ans):\n",
    "                        return ans[:-1] == c\n",
    "                    else:\n",
    "                        return ans == c\n",
    "                \n",
    "    return False\n",
    "\n",
    "def has_c_as_consonant_char_before_ith_vowel_char(w, c, i):\n",
    "    vowels = {'A', 'E', 'I', 'O', 'U'}\n",
    "    count = 0\n",
    "    j = 0\n",
    "    while j < len(w):\n",
    "        if w[j] in vowels:\n",
    "            count += 1\n",
    "            \n",
    "            if count == i:\n",
    "                if j == 0:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return w[j-1] == c\n",
    "            \n",
    "            \n",
    "            while w[j] in vowels:\n",
    "                j += 1\n",
    "                if j > len(w) - 1:\n",
    "                    return False\n",
    "        else:\n",
    "            j += 1\n",
    "            \n",
    "    return False\n",
    "\n",
    "def has_c_as_consonant_char_after_ith_vowel_char(w, c, i):\n",
    "    vowels = {'A', 'E', 'I', 'O', 'U'}\n",
    "    count = 0\n",
    "    j = 0\n",
    "    while j < len(w):\n",
    "        if w[j] in vowels:\n",
    "            count += 1\n",
    "            while w[j] in vowels:\n",
    "                j += 1\n",
    "                if j > len(w) - 1:\n",
    "                    return False\n",
    "            \n",
    "                    \n",
    "            if count == i:\n",
    "                if j > len(w) - 1:\n",
    "                    return False\n",
    "                else:\n",
    "                    return w[j] == c\n",
    "        else:\n",
    "            j += 1\n",
    "    return False\n",
    "\n",
    "def find_stress(p):\n",
    "    l = [syllable for syllable in p.split(' ') if is_vowel(syllable)]\n",
    "    for i in range(len(l)):\n",
    "        if '1' in l[i]:\n",
    "            return i + 1\n",
    "\n",
    "def nb_of_vowel_char_combination(w, i):\n",
    "    vowels = {'A', 'E', 'I', 'O', 'U'}\n",
    "    res = []\n",
    "    j = 0\n",
    "    while j < len(w):\n",
    "        l = []\n",
    "        if w[j] in vowels:\n",
    "            while w[j] in vowels:\n",
    "                l.append(w[j])\n",
    "                j += 1\n",
    "                if j >= len(w) - 1:\n",
    "                    break\n",
    "            res.append(''.join(l))\n",
    "        else:\n",
    "            j += 1\n",
    "    return i == len(res)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words, pronounciations = [], []\n",
    "df = pd.DataFrame()\n",
    "file = open('asset/training_data.txt')\n",
    "for line in file:\n",
    "    w, p = line.split(':')\n",
    "    p = p.rstrip('\\n')\n",
    "    words.append(w)\n",
    "    pronounciations.append(p)\n",
    "\n",
    "df['w'] = words\n",
    "df['p'] = pronounciations\n",
    "\n",
    "for v in vowel_list:\n",
    "    for i in range(1, 5):\n",
    "        df[v+str(i)] = df['p'].apply(lambda p: has_v_as_ith_vowel_syllable(p, v, i))\n",
    "        \n",
    "for s in syllable_list:\n",
    "    for i in range(1, 5):\n",
    "        for j in range(1, 3):\n",
    "            name = str(s) + str(j) + 'before' + str(i) + 'vowel syllable'\n",
    "            df[name] = df['p'].apply(lambda p: n_syllable_before_ith_vowel_syllable(p, s, i, j))\n",
    "\n",
    "for s in syllable_list:\n",
    "    for i in range(1, 5):\n",
    "        for j in range(1, 3):\n",
    "            name = str(s) + str(j) + 'after' + str(i) + 'vowel syllable'\n",
    "            df[name] = df['p'].apply(lambda p: n_syllable_after_ith_vowel_syllable(p, s, i, j))\n",
    "\n",
    "# for c in vowel_char_combinations:\n",
    "#     for i in range(1, 5):\n",
    "#         df[c+str(i)] = df['w'].apply(lambda w: has_c_as_ith_vowel_char_combination(w, c, i))        \n",
    "\n",
    "# for c in consonant_char_list:\n",
    "#     for i in range(1, 5):\n",
    "#         df[c+'before_'+str(i)+'_vowel_char'] = df['w'].apply(lambda w: has_c_as_consonant_char_before_ith_vowel_char(w, c, i))\n",
    "\n",
    "# for c in consonant_char_list:\n",
    "#     for i in range(1, 5):\n",
    "#         df[c+'after_'+str(i)+'_vowel_char'] = df['w'].apply(lambda w: has_c_as_consonant_char_after_ith_vowel_char(w, c, i))\n",
    "        \n",
    "\n",
    "df['2 vowel syllables'] = df['p'].apply(lambda p: len([i for i in p.split(' ') if is_vowel(i)]) == 2)\n",
    "df['3 vowel syllables'] = df['p'].apply(lambda p: len([i for i in p.split(' ') if is_vowel(i)]) == 3)\n",
    "df['4 vowel syllables'] = df['p'].apply(lambda p: len([i for i in p.split(' ') if is_vowel(i)]) == 4)\n",
    "\n",
    "df['stress'] = df['p'].apply(find_stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756217748207\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.96      0.94      6965\n",
      "          2       0.87      0.82      0.85      2462\n",
      "          3       0.89      0.72      0.79       568\n",
      "          4       0.50      0.40      0.44         5\n",
      "\n",
      "avg / total       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "features = list(df.columns)\n",
    "features.remove('w')\n",
    "features.remove('p')\n",
    "features.remove('stress')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "X_train = train[features]\n",
    "Y_train = train.stress\n",
    "\n",
    "X_test = test[features]\n",
    "Y_test = test.stress\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print(f1_score(Y_test, prediction, average='macro'))\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snowballstemmer\n",
    "stemmer = snowballstemmer.stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oversuppli'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stemWord(\"oversupplied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w'] = df['w'].apply(lambda w: str.lower(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>laviolette</td>\n",
       "      <td>L AE2 V IY0 OW0 L EH1 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>naivete</td>\n",
       "      <td>N AA0 IY2 V AH0 T EY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>arbitrageurs</td>\n",
       "      <td>AA2 R B AH0 T R AA2 ZH ER1 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>natividad</td>\n",
       "      <td>N AH2 T IH0 V IH0 D AA1 D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>santistevan</td>\n",
       "      <td>S AA2 N T IY0 S T EY0 V AA1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>inopportune</td>\n",
       "      <td>IH2 N AA2 P ER0 T UW1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>bellefeuille</td>\n",
       "      <td>B EH2 L AH0 F IY0 UW1 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>nitrosomines</td>\n",
       "      <td>N IH0 T R AA2 S AH0 M IY1 N Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8918</th>\n",
       "      <td>misrepresents</td>\n",
       "      <td>M IH0 S R EH2 P R AH0 Z EH1 N T S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>AA2 Z ER0 B AY0 JH AA1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>recitatives</td>\n",
       "      <td>R EH2 S AH0 T AH0 T IY1 V Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10369</th>\n",
       "      <td>mademoiselle</td>\n",
       "      <td>M AE2 D AH0 M AH0 Z EH1 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10419</th>\n",
       "      <td>nevertheless</td>\n",
       "      <td>N EH2 V ER0 DH AH0 L EH1 S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10828</th>\n",
       "      <td>electioneers</td>\n",
       "      <td>IH0 L EH2 K SH AH0 N IH1 R Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>misunderstands</td>\n",
       "      <td>M IH2 S AH0 N D ER0 S T AE1 N D Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10966</th>\n",
       "      <td>societe</td>\n",
       "      <td>S OW2 S IY0 EH0 T EY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11026</th>\n",
       "      <td>oversupplied</td>\n",
       "      <td>OW2 V ER0 S AH0 P L AY1 D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13875</th>\n",
       "      <td>overexposed</td>\n",
       "      <td>OW2 V ER0 IH0 K S P OW1 Z D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14047</th>\n",
       "      <td>aeroperu</td>\n",
       "      <td>EH2 R OW0 P EY0 R UW1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>underfinanced</td>\n",
       "      <td>AH2 N D ER0 F IH0 N AE1 N S T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15758</th>\n",
       "      <td>cabriolet</td>\n",
       "      <td>K AE2 B R IY0 OW0 L EY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17920</th>\n",
       "      <td>demodulate</td>\n",
       "      <td>D IY2 M AA2 JH AH0 L EY1 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18376</th>\n",
       "      <td>oversupply</td>\n",
       "      <td>OW2 V ER0 S AH0 P L AY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18503</th>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>AA2 N T R AH0 P R AH0 N ER1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21309</th>\n",
       "      <td>oversubscribe</td>\n",
       "      <td>OW2 V ER0 S AH0 B S K R AY1 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22655</th>\n",
       "      <td>misdiagnosed</td>\n",
       "      <td>M IH0 S D AY2 IH0 G N OW1 Z D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24313</th>\n",
       "      <td>nasional</td>\n",
       "      <td>N AE2 S IY0 AH0 N AE1 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>counterattacked</td>\n",
       "      <td>K AW2 N T ER0 AH0 T AE1 K T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24910</th>\n",
       "      <td>interviewees</td>\n",
       "      <td>IH2 N T ER0 V Y UW0 IY1 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25219</th>\n",
       "      <td>restaurateur</td>\n",
       "      <td>R EH2 S T ER0 AH0 T ER1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27648</th>\n",
       "      <td>reengineer</td>\n",
       "      <td>R IY0 EH2 N JH AH0 N IH1 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>balakumar</td>\n",
       "      <td>B AA2 L AH0 K UW0 M AA1 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30332</th>\n",
       "      <td>idiopath</td>\n",
       "      <td>IH2 D IY0 OW0 P AE1 TH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31655</th>\n",
       "      <td>overprotect</td>\n",
       "      <td>OW2 V ER0 P R AH0 T EH1 K T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32392</th>\n",
       "      <td>aperitif</td>\n",
       "      <td>AH0 P EH2 R AH0 T IY1 F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32463</th>\n",
       "      <td>misdiagnose</td>\n",
       "      <td>M IH0 S D AY2 IH0 G N OW1 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32769</th>\n",
       "      <td>sotomayor</td>\n",
       "      <td>S OW2 T OW0 M EY0 AO1 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33200</th>\n",
       "      <td>observateur</td>\n",
       "      <td>AA0 B Z ER2 V AH0 T UH1 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33509</th>\n",
       "      <td>espectador</td>\n",
       "      <td>EH0 S P EH2 K T AH0 D AO1 R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>industrielle</td>\n",
       "      <td>IH2 N D AH2 S T R IY0 EH1 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35792</th>\n",
       "      <td>legerdemain</td>\n",
       "      <td>L EH2 JH ER0 D AH0 M EY1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36216</th>\n",
       "      <td>overextends</td>\n",
       "      <td>OW2 V ER0 IH0 K S T EH1 N D Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36382</th>\n",
       "      <td>comedienne</td>\n",
       "      <td>K AH0 M IY2 D IY0 EH1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36768</th>\n",
       "      <td>teleconnect</td>\n",
       "      <td>T EH2 L AH0 K AH0 N EH1 K T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999</th>\n",
       "      <td>underreport</td>\n",
       "      <td>AH2 N D ER0 R IH0 P AO1 R T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37703</th>\n",
       "      <td>recriminate</td>\n",
       "      <td>R IH0 K R IH2 M IH0 N EY1 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38926</th>\n",
       "      <td>nitrosamines</td>\n",
       "      <td>N IH0 T R AA2 S AH0 M IY1 N Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39490</th>\n",
       "      <td>remunerate</td>\n",
       "      <td>R IH0 M Y UW2 N ER0 EY1 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40611</th>\n",
       "      <td>biodiverse</td>\n",
       "      <td>B AY2 OW0 D AY0 V ER1 S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44046</th>\n",
       "      <td>oversubscribed</td>\n",
       "      <td>OW2 V ER0 S AH0 B S K R AY1 B D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45430</th>\n",
       "      <td>ostpolitik</td>\n",
       "      <td>OW2 S T P OW2 L IH0 T IH1 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46849</th>\n",
       "      <td>sebastiane</td>\n",
       "      <td>S AH0 B AE2 S T IY0 EH1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47534</th>\n",
       "      <td>undersubscribed</td>\n",
       "      <td>AH2 N D ER0 S AH0 B S K R AY1 B D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47576</th>\n",
       "      <td>mujahideen</td>\n",
       "      <td>M UW2 JH AH0 HH EH0 D IY1 N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     w                                  p\n",
       "327         laviolette            L AE2 V IY0 OW0 L EH1 T\n",
       "1105           naivete              N AA0 IY2 V AH0 T EY1\n",
       "1725      arbitrageurs       AA2 R B AH0 T R AA2 ZH ER1 Z\n",
       "4253         natividad          N AH2 T IH0 V IH0 D AA1 D\n",
       "5016       santistevan      S AA2 N T IY0 S T EY0 V AA1 N\n",
       "5399       inopportune            IH2 N AA2 P ER0 T UW1 N\n",
       "8411      bellefeuille            B EH2 L AH0 F IY0 UW1 L\n",
       "8720      nitrosomines      N IH0 T R AA2 S AH0 M IY1 N Z\n",
       "8918     misrepresents  M IH0 S R EH2 P R AH0 Z EH1 N T S\n",
       "9162        azerbaijan           AA2 Z ER0 B AY0 JH AA1 N\n",
       "9562       recitatives        R EH2 S AH0 T AH0 T IY1 V Z\n",
       "10369     mademoiselle          M AE2 D AH0 M AH0 Z EH1 L\n",
       "10419     nevertheless         N EH2 V ER0 DH AH0 L EH1 S\n",
       "10828     electioneers       IH0 L EH2 K SH AH0 N IH1 R Z\n",
       "10858   misunderstands  M IH2 S AH0 N D ER0 S T AE1 N D Z\n",
       "10966          societe              S OW2 S IY0 EH0 T EY1\n",
       "11026     oversupplied          OW2 V ER0 S AH0 P L AY1 D\n",
       "13875      overexposed        OW2 V ER0 IH0 K S P OW1 Z D\n",
       "14047         aeroperu              EH2 R OW0 P EY0 R UW1\n",
       "14467    underfinanced      AH2 N D ER0 F IH0 N AE1 N S T\n",
       "15758        cabriolet            K AE2 B R IY0 OW0 L EY1\n",
       "17920       demodulate         D IY2 M AA2 JH AH0 L EY1 T\n",
       "18376       oversupply            OW2 V ER0 S AH0 P L AY1\n",
       "18503     entrepreneur        AA2 N T R AH0 P R AH0 N ER1\n",
       "21309    oversubscribe      OW2 V ER0 S AH0 B S K R AY1 B\n",
       "22655     misdiagnosed      M IH0 S D AY2 IH0 G N OW1 Z D\n",
       "24313         nasional            N AE2 S IY0 AH0 N AE1 L\n",
       "24328  counterattacked        K AW2 N T ER0 AH0 T AE1 K T\n",
       "24910     interviewees          IH2 N T ER0 V Y UW0 IY1 Z\n",
       "25219     restaurateur            R EH2 S T ER0 AH0 T ER1\n",
       "27648       reengineer         R IY0 EH2 N JH AH0 N IH1 R\n",
       "27977        balakumar          B AA2 L AH0 K UW0 M AA1 R\n",
       "30332         idiopath             IH2 D IY0 OW0 P AE1 TH\n",
       "31655      overprotect        OW2 V ER0 P R AH0 T EH1 K T\n",
       "32392         aperitif            AH0 P EH2 R AH0 T IY1 F\n",
       "32463      misdiagnose        M IH0 S D AY2 IH0 G N OW1 Z\n",
       "32769        sotomayor            S OW2 T OW0 M EY0 AO1 R\n",
       "33200      observateur          AA0 B Z ER2 V AH0 T UH1 R\n",
       "33509       espectador        EH0 S P EH2 K T AH0 D AO1 R\n",
       "34249     industrielle        IH2 N D AH2 S T R IY0 EH1 L\n",
       "35792      legerdemain         L EH2 JH ER0 D AH0 M EY1 N\n",
       "36216      overextends      OW2 V ER0 IH0 K S T EH1 N D Z\n",
       "36382       comedienne            K AH0 M IY2 D IY0 EH1 N\n",
       "36768      teleconnect        T EH2 L AH0 K AH0 N EH1 K T\n",
       "36999      underreport        AH2 N D ER0 R IH0 P AO1 R T\n",
       "37703      recriminate        R IH0 K R IH2 M IH0 N EY1 T\n",
       "38926     nitrosamines      N IH0 T R AA2 S AH0 M IY1 N Z\n",
       "39490       remunerate          R IH0 M Y UW2 N ER0 EY1 T\n",
       "40611       biodiverse            B AY2 OW0 D AY0 V ER1 S\n",
       "44046   oversubscribed    OW2 V ER0 S AH0 B S K R AY1 B D\n",
       "45430       ostpolitik        OW2 S T P OW2 L IH0 T IH1 K\n",
       "46849       sebastiane          S AH0 B AE2 S T IY0 EH1 N\n",
       "47534  undersubscribed  AH2 N D ER0 S AH0 B S K R AY1 B D\n",
       "47576       mujahideen        M UW2 JH AH0 HH EH0 D IY1 N"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['stress'] == 4][['w', 'p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns) - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15 * 4 + (39 * 16) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
